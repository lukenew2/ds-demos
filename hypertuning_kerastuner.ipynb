{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce28e9b",
   "metadata": {},
   "source": [
    "# HyperTuning with KerasTuner and TensorFlow\n",
    "---\n",
    "\n",
    "Building machine learning models is an iterative process that involves optimizing the model's performance and compute resources. The settings that you adjust during each iteration are called *hyperparameters*. They govern the training process and are held constant during training. \n",
    "\n",
    "The process of searching for optimal hyperparameters is called *hyperparameter tuning* or *hypertuning*, and is essential in any machine learning project. Hypertuning helps boost performance and reduces model complexity by removing unnecessary parameters (e.g., number of units in a dense layer).\n",
    "There are two type of hyperparameters:\n",
    "1. *Model hyperparameters* that influence model architecture (e.g., number and width of hidden layers in a DNN)\n",
    "2. *Algorithm hyperparameters* that influence the speed and quality of training (e.g., learning rate and activation function).\n",
    "\n",
    "The number of hyperparameter combinations, even in a shallow DNN, can grow insanely large making manually searching for the optimal set simply not feasible nor scalable. \n",
    "This post will introduce you to KerasTuner, a library made to automate the hyperparameter search. We'll build a deep learning model and train it on the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) with:\n",
    "* Pre-selected hyperparameters\n",
    "* Optimized hyperparameters with KerasTuner\n",
    "* Optimized pre-trained Xception and ResNet models\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f473c80",
   "metadata": {},
   "source": [
    "## Imports and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c6e3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.5.0\n",
      "KerasTuner Version: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"KerasTuner Version: {kt.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f976b2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================]ETA:  - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and split data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001a6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixels to values between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccdecb",
   "metadata": {},
   "source": [
    "## Baseline Performance\n",
    "Baseline performance will be judged by training a neural network with pre-selected hyperparameters:\n",
    "* `1` hidden layer with `512` neurons\n",
    "* `Adam` optimizer with learning rate of `0.001`\n",
    "* Dropout layer of `0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19ec4af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 19:30:22.950186: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build baseline model with Sequential API\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(keras.layers.Dense(units=512, activation='relu', name='dense_1'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9021d42f",
   "metadata": {},
   "source": [
    "Notice how we hardcoded each hyperparameter.  These include the number and width of hidden layers, activation function, and dropout.  \n",
    "\n",
    "We will now set the optimizer, learning rate, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bd13bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hype/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9dde6",
   "metadata": {},
   "source": [
    "With our model's setting defined, we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319ec5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 - 4s - loss: 0.1425 - accuracy: 0.9457 - val_loss: 0.4141 - val_accuracy: 0.8940\n",
      "Epoch 2/20\n",
      "1500/1500 - 4s - loss: 0.1391 - accuracy: 0.9463 - val_loss: 0.3898 - val_accuracy: 0.8983\n",
      "Epoch 3/20\n",
      "1500/1500 - 4s - loss: 0.1370 - accuracy: 0.9471 - val_loss: 0.3882 - val_accuracy: 0.8964\n",
      "Epoch 4/20\n",
      "1500/1500 - 4s - loss: 0.1401 - accuracy: 0.9466 - val_loss: 0.4148 - val_accuracy: 0.8958\n",
      "Epoch 5/20\n",
      "1500/1500 - 4s - loss: 0.1389 - accuracy: 0.9461 - val_loss: 0.4015 - val_accuracy: 0.8978\n",
      "Epoch 6/20\n",
      "1500/1500 - 4s - loss: 0.1363 - accuracy: 0.9492 - val_loss: 0.4282 - val_accuracy: 0.8961\n",
      "Epoch 7/20\n",
      "1500/1500 - 4s - loss: 0.1311 - accuracy: 0.9503 - val_loss: 0.4158 - val_accuracy: 0.8968\n",
      "Epoch 8/20\n",
      "1500/1500 - 4s - loss: 0.1293 - accuracy: 0.9505 - val_loss: 0.4085 - val_accuracy: 0.8907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ee9c03ac0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Early stopping set after 5 epochs\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=NUM_EPOCHS, validation_split=0.2, callbacks=[stop_early], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca719ec1",
   "metadata": {},
   "source": [
    "We'll create a helper function to evaluate our model and view the results in a dataframe helping us easily compare models later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e8f3468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - ETA: 6s - loss: 0.6888 - accuracy: 0.81 - ETA: 0s - loss: 0.4213 - accuracy: 0.88 - ETA: 0s - loss: 0.3893 - accuracy: 0.89 - ETA: 0s - loss: 0.3708 - accuracy: 0.89 - ETA: 0s - loss: 0.4258 - accuracy: 0.88 - ETA: 0s - loss: 0.4633 - accuracy: 0.88 - ETA: 0s - loss: 0.4742 - accuracy: 0.88 - ETA: 0s - loss: 0.4891 - accuracy: 0.88 - ETA: 0s - loss: 0.4854 - accuracy: 0.88 - ETA: 0s - loss: 0.4741 - accuracy: 0.88 - ETA: 0s - loss: 0.4683 - accuracy: 0.88 - 1s 2ms/step - loss: 0.4638 - accuracy: 0.8873\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    evaluate model on test set and show results in dataframe.\n",
    "    \n",
    "    model : keras model\n",
    "        trained keras model.\n",
    "    X_test : numpy array\n",
    "        Features of holdout set.\n",
    "    y_test : numpy array\n",
    "        Labels of holdout set.\n",
    "    \"\"\"\n",
    "    eval_dict = model.evaluate(X_test, y_test, return_dict=True)\n",
    "    \n",
    "    display_df = pd.DataFrame([eval_dict.values()], columns=[list(eval_dict.keys())])\n",
    "    \n",
    "    return display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a21cc202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - ETA: 6s - loss: 0.6888 - accuracy: 0.81 - ETA: 0s - loss: 0.4064 - accuracy: 0.89 - ETA: 0s - loss: 0.3804 - accuracy: 0.89 - ETA: 0s - loss: 0.4086 - accuracy: 0.88 - ETA: 0s - loss: 0.4621 - accuracy: 0.88 - ETA: 0s - loss: 0.4737 - accuracy: 0.88 - ETA: 0s - loss: 0.4859 - accuracy: 0.88 - ETA: 0s - loss: 0.4879 - accuracy: 0.88 - ETA: 0s - loss: 0.4730 - accuracy: 0.88 - ETA: 0s - loss: 0.4674 - accuracy: 0.88 - 1s 2ms/step - loss: 0.4638 - accuracy: 0.8873\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.463813</td>\n",
       "      <td>0.8873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss accuracy\n",
       "baseline  0.463813   0.8873"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "baseline_df.index = ['baseline']\n",
    "\n",
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfd181",
   "metadata": {},
   "source": [
    "There's the results for a single set of hyperparameters.  Imagine trying out different learning rates, dropout percentages, number of hidden layers, and number of neurons in each hidden layer.  As you can see, manual hypertuning is simply not feasible nor scalable.  In the next section you'll see how KerasTuner solves these problems simply by automating the process and searching the hyperparameter space in an efficient way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3eca70",
   "metadata": {},
   "source": [
    "## Keras Tuner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hype",
   "language": "python",
   "name": "hype"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
